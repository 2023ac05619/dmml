{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kfp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkfp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dsl\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m func_to_container_op\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kfp'"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op\n",
    "import pandas as pd\n",
    "import requests\n",
    "import boto3\n",
    "import sqlite3\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "from feast import FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Data Ingestion\n",
    "@func_to_container_op\n",
    "def fetch_transaction_data():\n",
    "    return pd.read_csv(\"transactions.csv\")\n",
    "\n",
    "@func_to_container_op\n",
    "def fetch_customer_data():\n",
    "    api_url = \"https://api.example.com/customers\"\n",
    "    response = requests.get(api_url)\n",
    "    return response.json()\n",
    "\n",
    "# 2. Raw Data Storage\n",
    "@func_to_container_op\n",
    "def upload_to_s3(transactions, customers):\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name = \"customer-churn-data\"\n",
    "\n",
    "    # Save ingested data locally first\n",
    "    transactions.to_csv(\"raw_transactions.csv\", index=False)\n",
    "    with open(\"raw_customer_data.json\", \"w\") as f:\n",
    "        f.write(str(customers))\n",
    "\n",
    "    # Upload files to S3\n",
    "    s3.upload_file(\"raw_transactions.csv\", bucket_name, \"raw/transactions.csv\")\n",
    "    s3.upload_file(\"raw_customer_data.json\", bucket_name, \"raw/customer_data.json\")\n",
    "\n",
    "    print(\"Files uploaded successfully to S3!\")\n",
    "\n",
    "# 3. Data Validation\n",
    "@func_to_container_op\n",
    "def validate_data():\n",
    "    df = pd.read_csv(\"raw_transactions.csv\")\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Validate column data types\n",
    "    expected_types = {'customer_id': int, 'transaction_amount': float, 'transaction_date': str}\n",
    "    for col, expected_type in expected_types.items():\n",
    "        if df[col].dtype != expected_type:\n",
    "            print(f\"Column {col} has incorrect data type\")\n",
    "\n",
    "    print(\"Data validation completed!\")\n",
    "\n",
    "# 4. Data Preparation\n",
    "@func_to_container_op\n",
    "def prepare_data():\n",
    "    df = pd.read_csv(\"raw_transactions.csv\")\n",
    "\n",
    "    # Handle missing values\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Convert date column to datetime\n",
    "    df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])\n",
    "\n",
    "    # Feature Engineering: Total spend per customer\n",
    "    customer_spend = df.groupby(\"customer_id\")[\"transaction_amount\"].sum().reset_index()\n",
    "    customer_spend.to_csv(\"cleaned_data.csv\", index=False)\n",
    "\n",
    "    print(\"Data preparation completed!\")\n",
    "\n",
    "# 5. Data Transformation and Storage\n",
    "@func_to_container_op\n",
    "def transform_and_store_data():\n",
    "    conn = sqlite3.connect(\"customer_churn.db\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS transformed_data (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        total_spend FLOAT,\n",
    "        last_transaction_date DATE\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert Transformed Data\n",
    "    cursor.execute(\"INSERT INTO transformed_data (customer_id, total_spend, last_transaction_date) VALUES (?, ?, ?)\",\n",
    "                   (101, 250.50, \"2023-12-01\"))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Data inserted successfully!\")\n",
    "\n",
    "# 6. Feature Store\n",
    "@func_to_container_op\n",
    "def manage_feature_store():\n",
    "    store = FeatureStore(repo_path=\"feature_repo\")\n",
    "\n",
    "    # Retrieve features for model training\n",
    "    customer_features = store.get_online_features(\n",
    "        entity_rows=[{\"customer_id\": 101}],\n",
    "        features=[\"customer.total_spend\", \"customer.last_transaction_date\"]\n",
    "    ).to_dict()\n",
    "\n",
    "    print(customer_features)\n",
    "\n",
    "# 7. Model Building\n",
    "@func_to_container_op\n",
    "def build_model():\n",
    "    df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "    # Assume a churn label column exists\n",
    "    X = df[[\"total_spend\"]]\n",
    "    y = df[\"churn_label\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # Save the model\n",
    "    with open(\"churn_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# 8. Model Deployment\n",
    "@func_to_container_op\n",
    "def deploy_model():\n",
    "    app = Flask(__name__)\n",
    "    model = pickle.load(open(\"churn_model.pkl\", \"rb\"))\n",
    "\n",
    "    @app.route(\"/predict\", methods=[\"POST\"])\n",
    "    def predict():\n",
    "        data = request.json\n",
    "        prediction = model.predict([[data[\"total_spend\"]]])\n",
    "        return jsonify({\"churn_prediction\": int(prediction[0])})\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        app.run(debug=True)\n",
    "\n",
    "# Define the Kubeflow Pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"Customer Churn Pipeline\",\n",
    "    description=\"An end-to-end pipeline for predicting customer churn.\"\n",
    ")\n",
    "def customer_churn_pipeline():\n",
    "    # Step 1: Data Ingestion\n",
    "    transactions_task = fetch_transaction_data()\n",
    "    customers_task = fetch_customer_data()\n",
    "\n",
    "    # Step 2: Upload to S3\n",
    "    upload_task = upload_to_s3(transactions_task.output, customers_task.output)\n",
    "\n",
    "    # Step 3: Data Validation\n",
    "    validation_task = validate_data().after(upload_task)\n",
    "\n",
    "    # Step 4: Data Preparation\n",
    "    preparation_task = prepare_data().after(validation_task)\n",
    "\n",
    "    # Step 5: Data Transformation and Storage\n",
    "    transform_task = transform_and_store_data().after(preparation_task)\n",
    "\n",
    "    # Step 6: Feature Store\n",
    "    feature_store_task = manage_feature_store().after(transform_task)\n",
    "\n",
    "    # Step 7: Model Building\n",
    "    model_building_task = build_model().after(feature_store_task)\n",
    "\n",
    "    # Step 8: Model Deployment\n",
    "    deploy_task = deploy_model().after(model_building_task)\n",
    "\n",
    "# Compile and run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    kfp.compiler.Compiler().compile(customer_churn_pipeline, \"customer_churn_pipeline.yaml\")\n",
    "    print(\"Pipeline compiled successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
