{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7928e0a4-2af1-4bdb-b7b6-acb9dfa08fa8",
   "metadata": {},
   "source": [
    "# Churn Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f0d505-47b8-4a05-9b7b-f8f246428eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 => Downloaded:  {{channel:task=download-operation;name=data_path;type=system.Dataset@0.0.1;}}\n",
      "Step2 => Ingested:  {{channel:task=ingestion-operation;name=output_csv;type=system.Dataset@0.0.1;}}\n",
      "Step3 => Processed X:  {{channel:task=processing-operation;name=processed_X;type=system.Dataset@0.0.1;}} \n",
      "\t Processed y:  {{channel:task=processing-operation;name=processed_y;type=system.Dataset@0.0.1;}}\n",
      "Step4 => Trained Model KNN:  {{channel:task=training-operation;name=knn_model;type=system.Dataset@0.0.1;}} \n",
      "\t Trained Model LG:  {{channel:task=training-operation;name=lg_model;type=system.Dataset@0.0.1;}} \n",
      "\t Trained Model SVM:  {{channel:task=training-operation;name=svm_model;type=system.Dataset@0.0.1;}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/3a6ef408-99ed-4e3a-884a-a04d2710ea8f\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/af1db035-d1db-4514-84e1-ebda8755996f\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath, pipeline, component\n",
    "\n",
    "\n",
    "# Define the data download component\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"requests==2.32.3\", \"minio==7.2.15\"]\n",
    ")\n",
    "def download_operation(url: str, data_path: OutputPath('Dataset')) -> None:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    if url != '':\n",
    "        # Pull object from MINIO\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        from io import StringIO\n",
    "        df = pd.read_csv(StringIO(response.text), header=0, sep=\";\")\n",
    "    else:        \n",
    "        from minio import Minio\n",
    "        # MinIO client setup\n",
    "        minio_client = Minio(\n",
    "            endpoint=\"192.168.203.181:30900\", \n",
    "            access_key=\"minioadmin\", \n",
    "            secret_key=\"minioadmin\", \n",
    "            secure=False\n",
    "        )    \n",
    "        # Get the object and save locally\n",
    "        minio_client.fget_object(\n",
    "            bucket_name=\"datasets\", \n",
    "            object_name=\"customer_churn_dataset-testing-copy.csv\",\n",
    "            file_path=\"/tmp/dataset.csv\"\n",
    "        )    \n",
    "        # Use the downloaded CSV file for data ingestion\n",
    "        df = pd.read_csv(\"/tmp/dataset.csv\")\n",
    "\n",
    "    if not df.empty:\n",
    "        print(\"Data Description => \",df.describe())\n",
    "        df.to_csv(data_path, index=False)\n",
    "\n",
    "# Define the data ingestion component\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\"]\n",
    ")\n",
    "def ingestion_operation(input_csv: InputPath('Dataset'), output_csv: OutputPath('Dataset')) -> None:\n",
    "        import pandas as pd\n",
    "        # Simulate loading data from CSV file located at a known path\n",
    "        # input_csv='customer_churn_dataset-testing-master.csv'\n",
    "        df = pd.read_csv(input_csv)\n",
    "        df.describe()\n",
    "        df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Define the data processing component\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"scikit-learn==1.6.1\"]\n",
    ")\n",
    "def processing_operation(input_csv: InputPath('Dataset'), processed_X: OutputPath('Dataset'), processed_y: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import SelectKBest, f_classif\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(input_csv)\n",
    "    print(\"__________Data Information__________\")\n",
    "    print(df.info())\n",
    "    print(\"__________Contract Length__________\")\n",
    "    print(df[\"Contract Length\"].value_counts())\n",
    "\n",
    "    # Feature selection and standardization\n",
    "    gender_map = {'Male': 0, 'Female': 1}\n",
    "    subscription_map = {'Basic': 0, 'Premium': 1, 'Pro': 2}\n",
    "    Contract_Length = {'Annual': 0, 'Quarterly': 1, 'Monthly' : 2} \n",
    "    \n",
    "    df['Gender'] = df['Gender'].map(gender_map)\n",
    "    df['Subscription Type'] = df['Subscription Type'].map(subscription_map)\n",
    "    df['Contract Length'] = df['Contract Length'].map(Contract_Length)\n",
    "\n",
    "    # Fill NaN values with the mode for each column\n",
    "    for column in df.columns:\n",
    "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "\n",
    "    threshold = 0.03\n",
    "    correlation_matrix = df.corr()\n",
    "    high_corr_features = correlation_matrix.index[abs(correlation_matrix[\"Churn\"]) > threshold].tolist()\n",
    "    high_corr_features.remove(\"Churn\")    \n",
    "    print(\"__________High Correlated Features__________\")\n",
    "    print(high_corr_features)\n",
    "    \n",
    "    X_selected = df[high_corr_features]\n",
    "    y_selected = df[\"Churn\"]\n",
    "\n",
    "    print(\"__________X-Data Information__________\")\n",
    "    print(pd.DataFrame(X_selected).info())\n",
    "    print(\"__________Y-Data Information__________\")\n",
    "    print(pd.Series(y_selected).info())\n",
    "\n",
    "    # # Select features\n",
    "    # selector = SelectKBest(score_func=f_classif, k=10)\n",
    "    # X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "    # # Standardization\n",
    "    # scaler = StandardScaler()\n",
    "    # X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # # Save the processed data to files\n",
    "    # pd.DataFrame(X_scaled).to_csv(processed_X, index=False)\n",
    "    # pd.Series(y).to_csv(processed_y, index=False)\n",
    "\n",
    "    #Save the processed data to files    \n",
    "    pd.DataFrame(X_selected).to_csv(processed_X, index=False)\n",
    "    pd.Series(y_selected).to_csv(processed_y, index=False)\n",
    "   \n",
    "\n",
    "# Define the model training component\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"scikit-learn==1.6.1\", \"joblib==1.4.2\"]\n",
    ")\n",
    "def training_operation(processed_X: InputPath('Dataset'), processed_y: InputPath('Dataset'), \n",
    "                      knn_model: OutputPath('Dataset'), lg_model: OutputPath('Dataset'), svm_model: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    import joblib\n",
    "\n",
    "    X_processed = pd.read_csv(processed_X)\n",
    "    y_processed = pd.read_csv(processed_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size= 0.2 , shuffle=True, random_state=55)\n",
    "    \n",
    "    print(\"__________X-Training Data Information__________\")\n",
    "    print(pd.DataFrame(X_train).info())\n",
    "    print(\"__________Y-Training Data Information__________\")\n",
    "    print(pd.DataFrame(y_train).info())\n",
    "    \n",
    "    #KNN Model \n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "    print(\"__________KNN Accuracy Score__________\")\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')    \n",
    "    print(\"__________KNN Classification Report__________\")\n",
    "    print(classification_report(y_test, y_pred_knn))\n",
    "    # Save the trained model\n",
    "    joblib.dump(knn, knn_model)\n",
    "\n",
    "    #Logistic Regression\n",
    "    lg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lg.fit(X_train, y_train)\n",
    "    y_pred_lg = lg.predict(X_test)\n",
    "    print(\"__________LR Accuracy Score__________\")\n",
    "    print(accuracy_score(y_test, y_pred_lg))\n",
    "    # Save the trained model\n",
    "    joblib.dump(lg, lg_model)\n",
    "\n",
    "    #SVM \n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    print(\"__________SVM Accuracy Score__________\")\n",
    "    print(accuracy_score(y_test, y_pred_svm))\n",
    "    # Save the trained model\n",
    "    joblib.dump(svm, svm_model)\n",
    "    \n",
    "\n",
    "# Define the pipeline\n",
    "@pipeline(\n",
    "    name='Customer Churn Prediction Pipeline',\n",
    "    description='A pipeline to perform customer churn prediction.'\n",
    ")\n",
    "def churn_prediction_pipeline(url: str):\n",
    "    # Step 1\n",
    "    download = download_operation(url=url)\n",
    "    print(\"Step1 => Downloaded: \", download.outputs['data_path'])\n",
    "    \n",
    "    # Step 2\n",
    "    ingest = ingestion_operation(input_csv=download.outputs['data_path'])\n",
    "    print(\"Step2 => Ingested: \", ingest.outputs['output_csv'])\n",
    "    \n",
    "    # Step 3\n",
    "    process = processing_operation(input_csv=ingest.outputs['output_csv'])\n",
    "    print(\"Step3 => Processed X: \", process.outputs['processed_X'], \"\\n\\t Processed y: \",  process.outputs['processed_y'])\n",
    "    \n",
    "    # Step 4\n",
    "    train = training_operation(processed_X=process.outputs['processed_X'], processed_y=process.outputs['processed_y'])    \n",
    "    print(\"Step4 => Trained Model KNN: \", train.outputs['knn_model'], \"\\n\\t Trained Model LG: \",  \n",
    "          train.outputs['lg_model'], \"\\n\\t Trained Model SVM: \",  train.outputs['svm_model'] )\n",
    "\n",
    "\n",
    "# Compile and run the pipeline\n",
    "if __name__ == '__main__':\n",
    "    # Compile the pipeline into a package\n",
    "    kfp.compiler.Compiler().compile(churn_prediction_pipeline, 'churn_prediction_pipeline.yaml')\n",
    "    \n",
    "    # Connect to Kubeflow Pipelines and execute the pipeline\n",
    "    client = kfp.Client()\n",
    "    url = ''\n",
    "    client.create_run_from_pipeline_func(churn_prediction_pipeline, arguments={'url': url}, enable_caching=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
