{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7928e0a4-2af1-4bdb-b7b6-acb9dfa08fa8",
   "metadata": {},
   "source": [
    "# Churn Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f0d505-47b8-4a05-9b7b-f8f246428eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 => Ingested:  {{channel:task=data-ingestion;name=output_csv;type=system.Dataset@0.0.1;}}\n",
      "Step2 => Processed X:  {{channel:task=data-processing;name=processed_X;type=system.Dataset@0.0.1;}} \n",
      "\t Processed y:  {{channel:task=data-processing;name=processed_y;type=system.Dataset@0.0.1;}}\n",
      "Step3 => Trained Model KNN:  {{channel:task=model-training;name=knn_model;type=system.Dataset@0.0.1;}} \n",
      "\t Trained Model LG:  {{channel:task=model-training;name=lg_model;type=system.Dataset@0.0.1;}} \n",
      "\t Trained Model SVM:  {{channel:task=model-training;name=svm_model;type=system.Dataset@0.0.1;}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/3a6ef408-99ed-4e3a-884a-a04d2710ea8f\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/1faead13-0ac5-40a0-b1dc-aaade1a75364\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath, pipeline, component\n",
    "\n",
    "# DOWNLOAD DATA FROM APIs, SQL and CSV (via Minio)\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"requests==2.32.3\", \"minio==7.2.15\", \"sqlalchemy==2.0.38\", \"pymysql==1.1.1\"]\n",
    ")\n",
    "def data_ingestion(input_csv: str, api_endpoint: str, sql_details: dict, output_csv: OutputPath('Dataset')) -> None:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import create_engine\n",
    "    from minio import Minio\n",
    "    from io import StringIO\n",
    "    \n",
    "    # Starting with an empty DataFrame\n",
    "    df = pd.DataFrame()  \n",
    "\n",
    "    # From API\n",
    "    if api_endpoint:          \n",
    "        # Send a GET Request and handle Error\n",
    "        try:\n",
    "            response = requests.get(api_endpoint) \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while sending GET request to {api_endpoint}: {e}\")\n",
    "            \n",
    "        # If there's a response, then load it into Datafrom    \n",
    "        if response:  \n",
    "            response.raise_for_status()\n",
    "            df_api = pd.DataFrame()\n",
    "            df_api = pd.DataFrame(response.json())\n",
    "            if not df_api.empty:\n",
    "                print(\"Data from API => \", df_api.describe())\n",
    "                # Merge all Dataframes into one\n",
    "                df = df_api if df.empty else pd.merge(df, df_api, on='key', how='inner') \n",
    "                \n",
    "\n",
    "    # From SQL\n",
    "    if sql_details: \n",
    "        df_db = pd.DataFrame()        \n",
    "        # Create a SQL engine for connection\n",
    "        engine = create_engine(sql_details['connection_string'], \n",
    "                               pool_size=10,\n",
    "                               max_overflow=20, \n",
    "                               pool_timeout=30, \n",
    "                               pool_recycle=1800, \n",
    "                               pool_pre_ping=True)\n",
    "        \n",
    "        # Run query to capture the data and handle Errors\n",
    "        try:\n",
    "            df_db = pd.read_sql(sql_details['query'], engine)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while querying the Database: {e}\") \n",
    "            \n",
    "        if not df_db.empty:\n",
    "            print(\"Data from Database => \", df_db.describe())\n",
    "            # Merge all Dataframes into one\n",
    "            df = df_db if df.empty else pd.merge(df, df_db, on='key', how='inner') \n",
    "            \n",
    "            \n",
    "            \n",
    "    # From CSV file (via Minio)\n",
    "    if input_csv:\n",
    "        df_minio = pd.DataFrame()\n",
    "        downloaded_file = ''\n",
    "        \n",
    "        # Create a minio client connection\n",
    "        minio_client = Minio(endpoint=\"192.168.203.181:30900\",\n",
    "                             access_key=\"minioadmin\",\n",
    "                             secret_key=\"minioadmin\",\n",
    "                             secure=False)        \n",
    "        \n",
    "        # Download the Object and handle Errors\n",
    "        try:\n",
    "            minio_client.fget_object(bucket_name=\"datasets\", \n",
    "                                     object_name=input_csv,\n",
    "                                     file_path='/tmp/dataset.csv')\n",
    "            downloaded_file = '/tmp/dataset.csv'\n",
    "        except Exception as e:\n",
    "                print(f\"An error occurred while downloading the file {input_csv}: {e}\")\n",
    "            \n",
    "        # If the CSV file is downloaded then load it into Datafrom    \n",
    "        if downloaded_file:   \n",
    "            df_minio = pd.read_csv(downloaded_file)\n",
    "            if not df_minio.empty:\n",
    "                print(\"Data from Minio => \", df_minio.describe())\n",
    "                # Merge all Dataframes into one\n",
    "                df = df_minio if df.empty else pd.merge(df, df_minio, on='key', how='inner')\n",
    "                \n",
    "\n",
    "    if not df.empty:\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(\"Downloaded Data => \", df.describe())\n",
    "        \n",
    "\n",
    "# Define the data processing component\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"scikit-learn==1.6.1\"]\n",
    ")\n",
    "def data_processing(input_csv: InputPath('Dataset'), processed_X: OutputPath('Dataset'), processed_y: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import SelectKBest, f_classif\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(input_csv)\n",
    "    print(\"__________Data Information__________\")\n",
    "    print(df.info())\n",
    "    print(\"__________Contract Length__________\")\n",
    "    print(df[\"Contract Length\"].value_counts())\n",
    "\n",
    "    # Feature selection and standardization\n",
    "    gender_map = {'Male': 0, 'Female': 1}\n",
    "    subscription_map = {'Basic': 0, 'Premium': 1, 'Pro': 2}\n",
    "    Contract_Length = {'Annual': 0, 'Quarterly': 1, 'Monthly' : 2} \n",
    "    \n",
    "    df['Gender'] = df['Gender'].map(gender_map)\n",
    "    df['Subscription Type'] = df['Subscription Type'].map(subscription_map)\n",
    "    df['Contract Length'] = df['Contract Length'].map(Contract_Length)\n",
    "\n",
    "    # Fill NaN values with the mode for each column\n",
    "    for column in df.columns:\n",
    "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "\n",
    "    threshold = 0.03\n",
    "    correlation_matrix = df.corr()\n",
    "    high_corr_features = correlation_matrix.index[abs(correlation_matrix[\"Churn\"]) > threshold].tolist()\n",
    "    high_corr_features.remove(\"Churn\")    \n",
    "    print(\"__________High Correlated Features__________\")\n",
    "    print(high_corr_features)\n",
    "    \n",
    "    X_selected = df[high_corr_features]\n",
    "    y_selected = df[\"Churn\"]\n",
    "\n",
    "    print(\"__________X-Data Information__________\")\n",
    "    print(pd.DataFrame(X_selected).info())\n",
    "    print(\"__________Y-Data Information__________\")\n",
    "    print(pd.Series(y_selected).info())\n",
    "\n",
    "    # # Select features\n",
    "    # selector = SelectKBest(score_func=f_classif, k=10)\n",
    "    # X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "    # # Standardization\n",
    "    # scaler = StandardScaler()\n",
    "    # X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # # Save the processed data to files\n",
    "    # pd.DataFrame(X_scaled).to_csv(processed_X, index=False)\n",
    "    # pd.Series(y).to_csv(processed_y, index=False)\n",
    "\n",
    "    #Save the processed data to files    \n",
    "    pd.DataFrame(X_selected).to_csv(processed_X, index=False)\n",
    "    pd.Series(y_selected).to_csv(processed_y, index=False)\n",
    "   \n",
    "\n",
    "# Define the model training component\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"scikit-learn==1.6.1\", \"joblib==1.4.2\"]\n",
    ")\n",
    "def model_training(processed_X: InputPath('Dataset'), processed_y: InputPath('Dataset'), \n",
    "                      knn_model: OutputPath('Dataset'), lg_model: OutputPath('Dataset'), svm_model: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    import joblib\n",
    "\n",
    "    X_processed = pd.read_csv(processed_X)\n",
    "    y_processed = pd.read_csv(processed_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size= 0.2 , shuffle=True, random_state=55)\n",
    "    \n",
    "    print(\"__________X-Training Data Information__________\")\n",
    "    print(pd.DataFrame(X_train).info())\n",
    "    print(\"__________Y-Training Data Information__________\")\n",
    "    print(pd.DataFrame(y_train).info())\n",
    "    \n",
    "    #KNN Model \n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "    print(\"__________KNN Accuracy Score__________\")\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')    \n",
    "    print(\"__________KNN Classification Report__________\")\n",
    "    print(classification_report(y_test, y_pred_knn))\n",
    "    # Save the trained model\n",
    "    joblib.dump(knn, knn_model)\n",
    "\n",
    "    #Logistic Regression\n",
    "    lg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lg.fit(X_train, y_train)\n",
    "    y_pred_lg = lg.predict(X_test)\n",
    "    print(\"__________LR Accuracy Score__________\")\n",
    "    print(accuracy_score(y_test, y_pred_lg))\n",
    "    # Save the trained model\n",
    "    joblib.dump(lg, lg_model)\n",
    "\n",
    "    #SVM \n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    print(\"__________SVM Accuracy Score__________\")\n",
    "    print(accuracy_score(y_test, y_pred_svm))\n",
    "    # Save the trained model\n",
    "    joblib.dump(svm, svm_model)\n",
    "    \n",
    "\n",
    "# Define the pipeline\n",
    "@pipeline(\n",
    "    name='Customer Churn Prediction Pipeline',\n",
    "    description='A pipeline to perform customer churn prediction.'\n",
    ")\n",
    "def churn_prediction_pipeline(input_csv: str, api_endpoint: str, sql_details: dict):\n",
    "    # Step 1\n",
    "    ingest = data_ingestion(input_csv=input_csv, api_endpoint=api_endpoint, sql_details=sql_details)\n",
    "    print(\"Step1 => Ingested: \", ingest.outputs['output_csv'])\n",
    "    \n",
    "    # Step 2\n",
    "    process = data_processing(input_csv=ingest.outputs['output_csv'])\n",
    "    print(\"Step2 => Processed X: \", process.outputs['processed_X'], \"\\n\\t Processed y: \",  process.outputs['processed_y'])\n",
    "    \n",
    "    # Step 3\n",
    "    train = model_training(processed_X=process.outputs['processed_X'], processed_y=process.outputs['processed_y'])    \n",
    "    print(\"Step3 => Trained Model KNN: \", train.outputs['knn_model'], \"\\n\\t Trained Model LG: \",  \n",
    "          train.outputs['lg_model'], \"\\n\\t Trained Model SVM: \",  train.outputs['svm_model'] )\n",
    "\n",
    "\n",
    "# Compile and run the pipeline\n",
    "if __name__ == '__main__':\n",
    "    # Compile the pipeline into a package\n",
    "    kfp.compiler.Compiler().compile(churn_prediction_pipeline, 'churn_prediction_pipeline.yaml')\n",
    "    \n",
    "    # Connect to Kubeflow Pipelines and execute the pipeline\n",
    "    client = kfp.Client()\n",
    "    api_endpoint = ''\n",
    "    input_csv = 'customer_churn_dataset-testing-copy.csv'\n",
    "    sql_params = {'connection_string': 'mysql+pymysql://app:TOwVvKU9yVsFj4xkaoLoEpKwmGso5GHkMLh9RRO32ma0xMNhKBR2THGUlwg68Yxd@192.168.203.181:30543/fin-db', 'query': 'SELECT * FROM accounts LIMIT 100'}\n",
    "    client.create_run_from_pipeline_func(churn_prediction_pipeline, arguments={'input_csv':input_csv, 'api_endpoint':api_endpoint, 'sql_details': sql_params}, enable_caching=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
