{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5310a9-36d2-49af-9dbe-84e5d29acdda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f0f411a-b8e8-4b61-94c2-9d31586fcfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 => Ingested:  {{channel:task=data-ingestion;name=output_csv;type=system.Dataset@0.0.1;}}\n",
      "\tEDA:  {{channel:task=data-eda;name=eda;type=system.Dataset@0.0.1;}} \n",
      "\tPlot:  {{channel:task=data-eda;name=plot;type=system.Dataset@0.0.1;}}\n",
      "Step2 => Processed X:  {{channel:task=data-processing;name=processed_X;type=system.Dataset@0.0.1;}} \n",
      "\t Processed y:  {{channel:task=data-processing;name=processed_y;type=system.Dataset@0.0.1;}}\n",
      "Step3 => Trained Model KNN:  {{channel:task=model-training;name=knn_model;type=system.Dataset@0.0.1;}} \n",
      "\t Trained Model LG:  {{channel:task=model-training;name=lg_model;type=system.Dataset@0.0.1;}} \n",
      "\t Trained Model SVM:  {{channel:task=model-training;name=svm_model;type=system.Dataset@0.0.1;}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/3a6ef408-99ed-4e3a-884a-a04d2710ea8f\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/029861d4-780f-4fa7-9e34-b656d55039ae\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Output, HTML, InputPath, OutputPath, pipeline, component\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Create Model directories\n",
    "os.makedirs(\"model_tracking\", exist_ok=True)\n",
    "\n",
    "# Setup Logging\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "logging.basicConfig(filename=\"logs/pipeline.log\",\n",
    "                    level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "\n",
    "# DOWNLOAD DATA FROM APIs, SQL and CSV (via Minio)\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"requests==2.32.3\", \"minio==7.2.15\", \"sqlalchemy==2.0.38\", \"pymysql==1.1.1\", \"psycopg2==2.9.10\"]\n",
    ")\n",
    "def data_ingestion(input_csv: str, api_endpoint: str, sql_details: dict, output_csv: OutputPath('Dataset')) -> None:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import psycopg2\n",
    "    from minio import Minio\n",
    "    import logging\n",
    "    import time\n",
    "\n",
    "\n",
    "    def merge_dataframes(df1, df2):\n",
    "        \n",
    "        common_columns = df1.columns.intersection(df2.columns)\n",
    "        # Check if there is at least one COMMON Column to use as a Key\n",
    "        if common_columns.empty:\n",
    "            logging.error(\"No common columns found to perform the merge.\")\n",
    "            logging.info(f\"\\n___Returning___\\n'{df1.info()}'\")\n",
    "            return df1\n",
    "        \n",
    "        key = common_columns[0]\n",
    "        # Propagate Data Types from df1 to df2 for common columns\n",
    "        for column in common_columns:\n",
    "            df2[column] = df2[column].astype(df1[column].dtype)\n",
    "            logging.info(f\"Converted column '{column}' in df2 to match df1's type ({df1[column].dtype})\")\n",
    "        \n",
    "        # Adding Missing Columns as None\n",
    "        for col in df2.columns:\n",
    "            if col not in df1.columns:\n",
    "                df1[col] = None\n",
    "        for col in df1.columns:\n",
    "            if col not in df2.columns:\n",
    "                df2[col] = None\n",
    "                \n",
    "        # Perform an Outer Merge\n",
    "        # merged = pd.merge(df1, df2, on=key, how='outer')\n",
    "        # print(\"Performed outer merge\")\n",
    "        \n",
    "        # Using combine_first to merge on index\n",
    "        df1_indexed = df1.set_index(key)\n",
    "        df2_indexed = df2.set_index(key)\n",
    "        merged = df1_indexed.combine_first(df2_indexed).reset_index()\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    # Create an empty Dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # From API\n",
    "    if api_endpoint:\n",
    "        try:\n",
    "            response = requests.get(api_endpoint)\n",
    "            # response.raise_for_status()\n",
    "            # Load JSON response into Datafrom\n",
    "            df_api = pd.DataFrame(response.json())\n",
    "            if not df_api.empty:\n",
    "                logging.info(\"\\n___Data from API___\\n%s\", df_api.describe())\n",
    "                # Merge all Dataframes into one\n",
    "                df = df_api if df.empty else merge_dataframes(df_api, df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(\"Error while fetching API data: %s\", e)\n",
    "\n",
    "    # From SQL  \n",
    "    if sql_details:\n",
    "        retries = 3\n",
    "        while retries > 0:\n",
    "            try:\n",
    "                # Connect to the PostgreSQL database using psycopg2\n",
    "                conn = psycopg2.connect(\n",
    "                    host=sql_details['DB_HOST'],\n",
    "                    port=sql_details['DB_PORT'],\n",
    "                    dbname=sql_details['DB_NAME'],\n",
    "                    user=sql_details['DB_USER'],\n",
    "                    password=sql_details['DB_PASSWORD']\n",
    "                )\n",
    "                cursor = conn.cursor()\n",
    "    \n",
    "                # Execute the query and fetch the data into a DataFrame\n",
    "                cursor.execute(sql_details['query'])\n",
    "                # Fetch all rows from the executed query\n",
    "                data = cursor.fetchall()\n",
    "                # Get column names from the cursor\n",
    "                colnames = [desc[0] for desc in cursor.description]\n",
    "                # Create DataFrame from the fetched data\n",
    "                df_db = pd.DataFrame(data, columns=colnames)\n",
    "    \n",
    "                if not df_db.empty:\n",
    "                    logging.info(\"\\n___Data from Database___\\n %s\", df_db.describe())\n",
    "                    # Merge all Dataframes into one\n",
    "                    df = df_db if df.empty else merge_dataframes(df_db, df)\n",
    "    \n",
    "                # Close the cursor and connection\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "    \n",
    "                # If successful, exit the loop\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error while querying Database: %s - Retries left: %d\", e, retries - 1)\n",
    "                retries -= 1\n",
    "                time.sleep(1)  # wait and retrying\n",
    "\n",
    "    # From CSV via Minio\n",
    "    if input_csv:\n",
    "        try:\n",
    "            minio_client = Minio(endpoint=\"192.168.203.181:30900\",\n",
    "                                 access_key=\"minioadmin\",\n",
    "                                 secret_key=\"minioadmin\",\n",
    "                                 secure=False)\n",
    "            minio_client.fget_object(bucket_name=\"datasets\",\n",
    "                                     object_name=input_csv,\n",
    "                                     file_path='/tmp/dataset.csv')\n",
    "            # If the CSV file is downloaded then load it into Datafrom\n",
    "            df_minio = pd.read_csv('/tmp/dataset.csv')\n",
    "            if not df_minio.empty:\n",
    "                logging.info(\"\\n___Data from Minio___\\n%s\", df_minio.describe())\n",
    "                # Merge all Dataframes into one\n",
    "                df = df_minio if df.empty else merge_dataframes(df_minio, df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(\"Error downloading file from Minio: %s\", e)\n",
    "\n",
    "    if not df.empty:\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        logging.info(\"\\n___Downloaded and merged data___\\n%s\", df.describe())\n",
    "        \n",
    "from kfp.dsl import component, InputPath, Output, HTML, OutputPath\n",
    "\n",
    "# EDA \n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"matplotlib==3.10.0\", \"seaborn==0.13.2\", \"ydata-profiling==4.12.2\", \"urllib3==1.26.20\"]\n",
    ")\n",
    "def data_eda(input_csv: InputPath('Dataset'), eda: OutputPath('Dataset'), plot: OutputPath('Dataset')) -> None:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from ydata_profiling import ProfileReport\n",
    "    import logging\n",
    "    import shutil\n",
    "    # import joblib\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Read the input dataset\n",
    "    df = pd.read_csv(input_csv)\n",
    "    base_dir = os.path.abspath(os.path.dirname(__file__))\n",
    "    eda_path = os.path.join(base_dir, \"eda\")\n",
    "    plot_path = os.path.join(base_dir, \"plot\")\n",
    "\n",
    "    # Creating the output directories \n",
    "    os.makedirs(eda_path, exist_ok=True)\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    logger.info(f\"Created directories: {eda_path}, {plot_path}\")\n",
    "\n",
    "    # Generate EDA report\n",
    "    profile = ProfileReport(df, title='EDA Report')\n",
    "    profile_report_path = os.path.join(os.path.dirname(eda), 'eda_report.html')\n",
    "    profile.to_file(profile_report_path)\n",
    "    logger.info(f\"EDA report saved to {profile_report_path}\")\n",
    "    # joblib.dump(os.path.join(eda_path, 'eda_report.html'), eda)\n",
    "    # Save the EDA report path\n",
    "    # with open(eda, 'w') as eda_file:\n",
    "    #     eda_file.write(profile_report_path)\n",
    "    shutil.copyfile(profile_report_path, eda)\n",
    "\n",
    "    # Example visualization: Churn distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x='Churn')\n",
    "    plt.title('Churn Distribution')\n",
    "\n",
    "    # Save the plot to the specified output path\n",
    "    plot_file_path = os.path.join(plot_path, 'churn_distribution.png')\n",
    "    plt.savefig(plot_file_path)\n",
    "    logger.info(f\"Plot saved to {plot_file_path}\")\n",
    "    # joblib.dump(plot_file_path, plot)\n",
    "    # Save the plot path\n",
    "    # with open(plot, 'w') as plot_file:\n",
    "    #     plot_file.write(plot_file_path)\n",
    "    shutil.copyfile(plot_file_path, plot)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "# Data Processing \n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"scikit-learn==1.6.1\", \"imbalanced-learn==0.13.0\"]\n",
    ")\n",
    "def data_processing(input_csv: InputPath('Dataset'), processed_X: OutputPath('Dataset'), processed_y: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Data Imputation and Cleaning\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "    categorical_cols = df.select_dtypes(include='object').columns\n",
    "    \n",
    "    # Impute missing values\n",
    "    df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "    df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    \n",
    "    logging.info(\"Unique values in 'Churn' before processing: %s\", df['Churn'].unique())\n",
    "    \n",
    "    # Ensure 'Churn' is numeric\n",
    "    if df['Churn'].dtype == 'object':\n",
    "        df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Drop rows where 'Churn' is NaN after mapping\n",
    "    df.dropna(subset=['Churn'], inplace=True)\n",
    "    \n",
    "    logging.info(\"Unique values in 'Churn' after mapping: %s\", df['Churn'].unique())\n",
    "    \n",
    "    # Feature Engineering\n",
    "    if 'CustomerID' in df.columns:\n",
    "        df = df.drop(columns=['CustomerID'])\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns='Churn')\n",
    "    y = df['Churn']\n",
    "    \n",
    "    # Verify no NaN values in the target variable\n",
    "    if y.isnull().any():\n",
    "        logging.error(\"Target variable y contains NaN values after processing.\")\n",
    "        return\n",
    "    \n",
    "    # Encode categorical columns using OneHotEncoder\n",
    "    categorical_cols = X.select_dtypes(include='object').columns\n",
    "    numeric_cols = X.select_dtypes(include='number').columns\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_cols),\n",
    "            ('cat', OneHotEncoder(), categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Transform the feature matrix\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Apply SMOTE for balancing\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_processed, y)\n",
    "    \n",
    "    # Save processed data\n",
    "    pd.DataFrame(X_resampled).to_csv(processed_X, index=False)\n",
    "    pd.Series(y_resampled).to_csv(processed_y, index=False)\n",
    "    \n",
    "    logging.info(\"Processed data saved with shapes: X: %s, y: %s\", X_resampled.shape, y_resampled.shape)\n",
    "    \n",
    "# Model Training Component\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"scikit-learn==1.6.1\", \"joblib==1.4.2\", \"mlflow==2.11.0\"]\n",
    ")\n",
    "def model_training(processed_X: InputPath('Dataset'), processed_y: InputPath('Dataset'), \n",
    "                   knn_model: OutputPath('Dataset'), lg_model: OutputPath('Dataset'), svm_model: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import joblib\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    import logging\n",
    "\n",
    "    # Start MLflow tracking\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "\n",
    "    mlflow.start_run()\n",
    "\n",
    "    X_processed = pd.read_csv(processed_X)\n",
    "    y_processed = pd.read_csv(processed_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=55)\n",
    "\n",
    "    # KNN Model\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "    logging.info(\"KNN Accuracy: %.2f\", knn_accuracy * 100)\n",
    "    joblib.dump(knn, knn_model)\n",
    "    mlflow.log_metric(\"KNN Accuracy\", knn_accuracy)\n",
    "    mlflow.sklearn.log_model(knn, \"knn_model\")\n",
    "\n",
    "    # Logistic Regression\n",
    "    lg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lg.fit(X_train, y_train)\n",
    "    y_pred_lg = lg.predict(X_test)\n",
    "    lg_accuracy = accuracy_score(y_test, y_pred_lg)\n",
    "    logging.info(\"Logistic Regression Accuracy: %.2f\", lg_accuracy * 100)\n",
    "    joblib.dump(lg, lg_model)\n",
    "    mlflow.log_metric(\"Logistic Regression Accuracy\", lg_accuracy)\n",
    "    mlflow.sklearn.log_model(lg, \"lg_model\")\n",
    "\n",
    "    # SVM\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "    logging.info(\"SVM Accuracy: %.2f\", svm_accuracy * 100)\n",
    "    joblib.dump(svm, svm_model)\n",
    "    mlflow.log_metric(\"SVM Accuracy\", svm_accuracy)\n",
    "    mlflow.sklearn.log_model(svm, \"svm_model\")\n",
    "\n",
    "    # End MLflow run\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Define the pipeline\n",
    "@pipeline(\n",
    "    name='Customer Churn Prediction Pipeline',\n",
    "    description='A pipeline to perform customer churn prediction.'\n",
    ")\n",
    "def churn_prediction_pipeline(input_csv: str, api_endpoint: str, sql_details: dict):\n",
    "    # Step 1 - Data Ingestion\n",
    "    ingest = data_ingestion(input_csv=input_csv, api_endpoint=api_endpoint, sql_details=sql_details)\n",
    "    print(\"Step1 => Ingested: \", ingest.outputs['output_csv'])\n",
    "    eda = data_eda(input_csv=ingest.outputs['output_csv'])\n",
    "    print(\"\\tEDA: \", eda.outputs['eda'], \"\\n\\tPlot: \", eda.outputs['plot'])\n",
    "    \n",
    "    # Step 3 - Data Processing\n",
    "    process = data_processing(input_csv=ingest.outputs['output_csv'])\n",
    "    print(\"Step2 => Processed X: \", process.outputs['processed_X'], \"\\n\\t Processed y: \",  process.outputs['processed_y'])\n",
    "    \n",
    "    # Step 3 - Model Training\n",
    "    train = model_training(processed_X=process.outputs['processed_X'], processed_y=process.outputs['processed_y'])\n",
    "    print(\"Step3 => Trained Model KNN: \", train.outputs['knn_model'], \"\\n\\t Trained Model LG: \",  \n",
    "          train.outputs['lg_model'], \"\\n\\t Trained Model SVM: \",  train.outputs['svm_model'] )\n",
    "    \n",
    "# Compile and run the pipeline\n",
    "if __name__ == '__main__':\n",
    "    # Compile the pipeline into a package\n",
    "    kfp.compiler.Compiler().compile(churn_prediction_pipeline, 'churn_prediction_pipeline.yaml')\n",
    "    \n",
    "    # Connect to Kubeflow Pipelines and execute the pipeline\n",
    "    client = kfp.Client()\n",
    "    \n",
    "    # Define API endpoint\n",
    "    api_endpoint = ''  \n",
    "\n",
    "    # CSV file\n",
    "    input_csv = 'customer_churn_dataset-testing-copy.csv'\n",
    "    \n",
    "    # Placeholder for SQL details\n",
    "    sql_params = {'DB_HOST': '192.168.203.181', 'DB_PORT': '30543', 'DB_NAME': 'fin-db', 'DB_USER': 'app',\n",
    "                  'DB_PASSWORD': 'TOwVvKU9yVsFj4xkaoLoEpKwmGso5GHkMLh9RRO32ma0xMNhKBR2THGUlwg68Yxd', \n",
    "                  'query': 'SELECT * FROM accounts LIMIT 100'}\n",
    "    client.create_run_from_pipeline_func(churn_prediction_pipeline,\n",
    "                                         arguments={'input_csv':input_csv,\n",
    "                                                    'api_endpoint':api_endpoint,\n",
    "                                                    'sql_details': sql_params},\n",
    "                                         enable_caching=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
