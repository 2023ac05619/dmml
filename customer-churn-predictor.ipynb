{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5310a9-36d2-49af-9dbe-84e5d29acdda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f0f411a-b8e8-4b61-94c2-9d31586fcfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 => Ingested:  {{channel:task=data-ingestion;name=output_csv;type=system.Dataset@0.0.1;}}\n",
      "Step2 => EDA Report :  {{channel:task=data-eda;name=eda_report;type=system.HTML@0.0.1;}} \n",
      "\t Plot Path:  {{channel:task=data-eda;name=plot_path;type=system.Artifact@0.0.1;}}\n",
      "Step2 => Processed X:  {{channel:task=data-processing;name=processed_X;type=system.Dataset@0.0.1;}} \n",
      "\t Processed y:  {{channel:task=data-processing;name=processed_y;type=system.Dataset@0.0.1;}}\n",
      "Step3 => Trained Model KNN:  {{channel:task=model-training;name=knn_model;type=system.Dataset@0.0.1;}} \n",
      "\t Trained Model LG:  {{channel:task=model-training;name=lg_model;type=system.Dataset@0.0.1;}} \n",
      "\t Trained Model SVM:  {{channel:task=model-training;name=svm_model;type=system.Dataset@0.0.1;}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/3a6ef408-99ed-4e3a-884a-a04d2710ea8f\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/6c98d9e2-b257-424c-82eb-aa16286e1e32\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath, OutputPath, pipeline, component\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Create Necessary Directories\n",
    "os.makedirs(\"model_tracking\", exist_ok=True)\n",
    "\n",
    "# Setup Logging\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "logging.basicConfig(filename=\"logs/pipeline.log\",\n",
    "                    level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "\n",
    "# DOWNLOAD DATA FROM APIs, SQL and CSV (via Minio)\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"requests==2.32.3\", \"minio==7.2.15\", \"sqlalchemy==2.0.38\", \"pymysql==1.1.1\", \"psycopg2==2.9.10\"]\n",
    ")\n",
    "def data_ingestion(input_csv: str, api_endpoint: str, sql_details: dict, output_csv: OutputPath('Dataset')) -> None:\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import psycopg2\n",
    "    from minio import Minio\n",
    "    import logging\n",
    "    import time\n",
    "\n",
    "    # Create an empty Dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # From API\n",
    "    if api_endpoint:\n",
    "        try:\n",
    "            response = requests.get(api_endpoint)\n",
    "            # response.raise_for_status()\n",
    "            # Load JSON response into Datafrom\n",
    "            df_api = pd.DataFrame(response.json())\n",
    "            if not df_api.empty:\n",
    "                logging.info(\"Data from API: %s\", df_api.describe())\n",
    "                # Merge all Dataframes into one\n",
    "                df = df_api if df.empty else pd.merge(df, df_api, on='key', how='inner')\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error while fetching API data: %s\", e)\n",
    "\n",
    "    # From SQL  \n",
    "    if sql_details:\n",
    "        retries = 3\n",
    "        while retries > 0:\n",
    "            try:\n",
    "                # Connect to the PostgreSQL database using psycopg2\n",
    "                conn = psycopg2.connect(\n",
    "                    host=sql_details['DB_HOST'],\n",
    "                    port=sql_details['DB_PORT'],\n",
    "                    dbname=sql_details['DB_NAME'],\n",
    "                    user=sql_details['DB_USER'],\n",
    "                    password=sql_details['DB_PASSWORD']\n",
    "                )\n",
    "                cursor = conn.cursor()\n",
    "    \n",
    "                # Execute the query and fetch the data into a DataFrame\n",
    "                cursor.execute(sql_details['query'])\n",
    "                # Fetch all rows from the executed query\n",
    "                data = cursor.fetchall()\n",
    "                # Get column names from the cursor\n",
    "                colnames = [desc[0] for desc in cursor.description]\n",
    "                # Create DataFrame from the fetched data\n",
    "                df_db = pd.DataFrame(data, columns=colnames)\n",
    "    \n",
    "                if not df_db.empty:\n",
    "                    logging.info(\"Data from Database: %s\", df_db.describe())\n",
    "                    # Merge all Dataframes into one\n",
    "                    df = df_db if 'df' not in locals() else pd.merge(df, df_db, on='key', how='inner')\n",
    "    \n",
    "                # Close the cursor and connection\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "    \n",
    "                # If successful, exit the loop\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error while querying Database: %s - Retries left: %d\", e, retries - 1)\n",
    "                retries -= 1\n",
    "                time.sleep(1)  # wait and retrying\n",
    "\n",
    "    # From CSV via Minio\n",
    "    if input_csv:\n",
    "        try:\n",
    "            minio_client = Minio(endpoint=\"192.168.203.181:30900\",\n",
    "                                 access_key=\"minioadmin\",\n",
    "                                 secret_key=\"minioadmin\",\n",
    "                                 secure=False)\n",
    "            minio_client.fget_object(bucket_name=\"datasets\",\n",
    "                                     object_name=input_csv,\n",
    "                                     file_path='/tmp/dataset.csv')\n",
    "            # If the CSV file is downloaded then load it into Datafrom\n",
    "            df_minio = pd.read_csv('/tmp/dataset.csv')\n",
    "            if not df_minio.empty:\n",
    "                logging.info(\"Data from Minio: %s\", df_minio.describe())\n",
    "                # Merge all Dataframes into one\n",
    "                df = df_minio if df.empty else pd.merge(df, df_minio, on='key', how='inner')                \n",
    "        except Exception as e:\n",
    "            logging.error(\"Error downloading file from Minio: %s\", e)\n",
    "\n",
    "    if not df.empty:\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        logging.info(\"Downloaded and merged data: %s\", df.describe())\n",
    "        \n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.3\",           # Downgrade pandas to a compatible version\n",
    "        \"matplotlib==3.10.0\",\n",
    "        \"seaborn==0.13.2\",\n",
    "        \"ydata-profiling==4.12.2\"   # Adjust this to the latest compatible version\n",
    "    ]\n",
    ")\n",
    "def data_eda(input_csv: InputPath('Dataset'), eda_report: OutputPath('HTML'), plot_path: OutputPath('Plot')) -> None:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from ydata_profiling import ProfileReport\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Generate EDA report\n",
    "    profile = ProfileReport(df, title='EDA Report')\n",
    "    profile.to_file(eda_report)\n",
    "    \n",
    "    # Example visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x='Churn')\n",
    "    plt.title('Churn Distribution')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "# Note: \"pandas-profiling\" version needs to be compatible with current pydantic. You might need to adjust versions if issues persist.\n",
    "    \n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"scikit-learn==1.6.1\", \"imblearn==0.0\"]\n",
    ")\n",
    "def data_processing(input_csv: InputPath('Dataset'), processed_X: OutputPath('Dataset'), processed_y: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import numpy as np\n",
    "    import logging\n",
    "\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Data Imputation and Cleaning\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    df.loc[:, df.select_dtypes(include='number').columns] = num_imputer.fit_transform(df.select_dtypes(include='number'))\n",
    "    df.loc[:, df.select_dtypes(include='object').columns] = cat_imputer.fit_transform(df.select_dtypes(include='object'))\n",
    "\n",
    "    # Encode 'Churn' column\n",
    "    if df['Churn'].isnull().any():\n",
    "        df['Churn'].fillna(df['Churn'].mode()[0], inplace=True)\n",
    "    df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    # Feature Engineering\n",
    "    if 'customer_id' in df.columns:\n",
    "        df = df.drop(columns=['customer_id'])\n",
    "\n",
    "    X = df.drop(columns='Churn')\n",
    "    y = df['Churn']\n",
    "\n",
    "    # Verify no NaN values in the target variable\n",
    "    if y.isnull().any():\n",
    "        logging.error(\"Target variable y contains NaN values after imputation.\")\n",
    "        return\n",
    "\n",
    "    # Apply SMOTE for balancing\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    pd.DataFrame(X_resampled).to_csv(processed_X, index=False)\n",
    "    pd.Series(y_resampled).to_csv(processed_y, index=False)\n",
    "    logging.info(\"Processed data saved with shapes: X: %s, y: %s\", X_resampled.shape, y_resampled.shape)\n",
    "    \n",
    "# Model Training Component\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas==2.2.3\", \"scikit-learn==1.6.1\", \"joblib==1.4.2\", \"mlflow==2.11.0\"]\n",
    ")\n",
    "def model_training(processed_X: InputPath('Dataset'), processed_y: InputPath('Dataset'), \n",
    "                   knn_model: OutputPath('Dataset'), lg_model: OutputPath('Dataset'), svm_model: OutputPath('Dataset')) -> None:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import joblib\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "\n",
    "    # Start MLflow tracking\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "\n",
    "    mlflow.start_run()\n",
    "\n",
    "    X_processed = pd.read_csv(processed_X)\n",
    "    y_processed = pd.read_csv(processed_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=55)\n",
    "\n",
    "    # KNN Model\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "    logging.info(\"KNN Accuracy: %.2f\", knn_accuracy * 100)\n",
    "    joblib.dump(knn, knn_model)\n",
    "    mlflow.log_metric(\"KNN Accuracy\", knn_accuracy)\n",
    "    mlflow.sklearn.log_model(knn, \"knn_model\")\n",
    "\n",
    "    # Logistic Regression\n",
    "    lg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lg.fit(X_train, y_train)\n",
    "    y_pred_lg = lg.predict(X_test)\n",
    "    lg_accuracy = accuracy_score(y_test, y_pred_lg)\n",
    "    logging.info(\"Logistic Regression Accuracy: %.2f\", lg_accuracy * 100)\n",
    "    joblib.dump(lg, lg_model)\n",
    "    mlflow.log_metric(\"Logistic Regression Accuracy\", lg_accuracy)\n",
    "    mlflow.sklearn.log_model(lg, \"lg_model\")\n",
    "\n",
    "    # SVM\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "    logging.info(\"SVM Accuracy: %.2f\", svm_accuracy * 100)\n",
    "    joblib.dump(svm, svm_model)\n",
    "    mlflow.log_metric(\"SVM Accuracy\", svm_accuracy)\n",
    "    mlflow.sklearn.log_model(svm, \"svm_model\")\n",
    "\n",
    "    # End MLflow run\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Define the pipeline\n",
    "@pipeline(\n",
    "    name='Customer Churn Prediction Pipeline',\n",
    "    description='A pipeline to perform customer churn prediction.'\n",
    ")\n",
    "def churn_prediction_pipeline(input_csv: str, api_endpoint: str, sql_details: dict):\n",
    "    # Step 1 - Data Ingestion\n",
    "    ingest = data_ingestion(input_csv=input_csv, api_endpoint=api_endpoint, sql_details=sql_details)\n",
    "    print(\"Step1 => Ingested: \", ingest.outputs['output_csv'])\n",
    "\n",
    "    # Step 2 - EDA\n",
    "    eda = data_eda(input_csv=ingest.outputs['output_csv']) \n",
    "    print(\"Step2 => EDA Report : \", eda.outputs['eda_report'], \"\\n\\t Plot Path: \", eda.outputs['plot_path'])\n",
    "    \n",
    "    # Step 3 - Data Processing\n",
    "    process = data_processing(input_csv=ingest.outputs['output_csv'])\n",
    "    print(\"Step2 => Processed X: \", process.outputs['processed_X'], \"\\n\\t Processed y: \",  process.outputs['processed_y'])\n",
    "    \n",
    "    # Step 4 - Model Training\n",
    "    train = model_training(processed_X=process.outputs['processed_X'], processed_y=process.outputs['processed_y'])\n",
    "    print(\"Step3 => Trained Model KNN: \", train.outputs['knn_model'], \"\\n\\t Trained Model LG: \",  \n",
    "          train.outputs['lg_model'], \"\\n\\t Trained Model SVM: \",  train.outputs['svm_model'] )\n",
    "    \n",
    "# Compile and run the pipeline\n",
    "if __name__ == '__main__':\n",
    "    # Compile the pipeline into a package\n",
    "    kfp.compiler.Compiler().compile(churn_prediction_pipeline, 'churn_prediction_pipeline.yaml')\n",
    "    \n",
    "    # Connect to Kubeflow Pipelines and execute the pipeline\n",
    "    client = kfp.Client()\n",
    "    \n",
    "    # Define API endpoint\n",
    "    api_endpoint = ''  \n",
    "\n",
    "    # CSV file\n",
    "    input_csv = 'customer_churn_dataset-testing-copy.csv'\n",
    "    \n",
    "    # Placeholder for SQL details\n",
    "    sql_params = {'DB_HOST': '192.168.203.181', 'DB_PORT': '30543', 'DB_NAME': 'fin-db', 'DB_USER': 'app',\n",
    "                  'DB_PASSWORD': 'TOwVvKU9yVsFj4xkaoLoEpKwmGso5GHkMLh9RRO32ma0xMNhKBR2THGUlwg68Yxd', \n",
    "                  'query': 'SELECT * FROM accounts LIMIT 100'}\n",
    "    client.create_run_from_pipeline_func(churn_prediction_pipeline,\n",
    "                                         arguments={'input_csv':input_csv,\n",
    "                                                    'api_endpoint':api_endpoint,\n",
    "                                                    'sql_details': sql_params},\n",
    "                                         enable_caching=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
